---
title: "Frequency Analysis of Notes From Portland Harbor Model Workshops"
author: "Curtis C. Bohlen"
date: "2022-12-09"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    fig_width: 5
    fig_height: 4
---

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:100px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
CBEP recently received a grant from NSF's CIVIC Innovation Challenge to work on 
developing hydrodynamic models that address community needs in Portland Harbor.
As part of the project, CBEP hosted three community workshops in November of
2022.

Facilitators produced both "live" notes during the meeting -- visible to all on
a screen at the front of the meeting room -- and detailed meeting transcripts.
CBEP staff then reviewed those notes paragraph by paragraph, and coded each 
paragraph in terms of six characteristics:

*  Potential users and uses of hydrodynamic models,

*  Data or information needs identified by community members,

*  Implied extensions of the initial Casco Bay Model required to fully address
   those data needs, and
   
*  Ideas for improving communications of model results (e.g., communications 
   channels and user interface design),

*  Specifications for model performance or capabilities such as resolution,
   geographic coverage or ability to conduct simulations.

*  Suggestions about monitoring or data collection that could improve
   information availability.

If a paragraph or live note included something relevant to one or more of
these categories, we summarized the related idea, and then assigned each
paragraph or comment to categories. In this way we cn look at what ideas were 
expressed most commonly during the workshops.

Of course, not all paragraphs include information related to each of the five 
types of information, so there is not a perfect one-to-one correspondence 
between categories.

In this R Notebook, I explore these data in terms of frequency with which
certain ideas came up, and cross-correlations among ideas.

# Load Packages
```{r}
library(tidyverse)
library(readxl)
library(networkD3)

theme_set(theme_classic())
```

# Create Figures Folder
```{r}
dir.create(file.path(getwd(), 'figures'), showWarnings = FALSE)
```


# Load Data
```{r}
the_data <- read_excel("Export_Data_Query.xlsx" ) %>%
  mutate(ID = as.integer(ID)) %>%
  rename_with(function(x) sub(" Category_Category", '_Category', x)) %>%
  rename_with(function(x) sub(" ", '_', x))
head(the_data)
```

Our coding was generated in a somewhat sloppy `Access` database, and because of
the way SQL works, it is easier to replace numerical values for some groups
here, in `R`, rather than before we exported the data from `Access`. I read in
the dictionaries here.

```{r}
timing_table <- read_excel("Timing Category.xlsx", 
    col_types = c("numeric", "text", "text"))
data_types_table <- read_excel("Data Type.xlsx", 
    col_types = c("numeric", "text", "text"))
```

And finally I correct the data table to all text entries.

```{r}
the_data <- the_data %>%
  mutate(Data_Timing = timing_table$Timing[match(Data_Timing, 
                                                 timing_table$ID)],
         Extension_Timing = timing_table$Timing[match(Extension_Timing,
                                                      timing_table$ID)]) %>%
  mutate(Data_Group = data_types_table$Group[match(Data_Group,
                                                    data_types_table$ID)],
         Monitoring_Data_Group = data_types_table$Group[match(Monitoring_Data_Group,
                                                    data_types_table$ID)] )
```

#A Warning about Uniqueness
We have to be careful here, because each note or comment can be represented in 
this data table multiple times. Each paragraph in the meeting transcript might 
imply several different users, for example.  But if there are multiple users and
multiple data types, the records got duplicated (in part) in the SQL query.  So 
for any analysis, we need to test for uniqueness of the data. always

We actually have over 400 records, built out of just over 200 unique comments.

```{r}
cat("All rows in the data:\t\t")
nrow(the_data)

cat("Unique comments reviewed:\t")
the_data %>%
  select(ID) %>%
  unique() %>%
  nrow()
```

# Users
```{r}
tmp <- the_data %>%
  select(ID, User_Category) %>%
  unique()
tst <- xtabs(~User_Category, tmp) %>%
  sort(decreasing = TRUE) %>%
  as_tibble()


cat("Number of Unique User Records:\t")
sum(tst$n)
```
```{r fig.height = 5, fig.width = 5}
tst %>%
  mutate(User_Category = fct_reorder(User_Category, n, .desc = TRUE)) %>%
  
  ggplot(aes(User_Category, n)) +
  geom_col(fill = "blue4") +
  theme(axis.text.x = element_text(angle = 90, size = 16,
                                   hjust = 1, vjust = 0.25)) +
  ylab('Count') +
  xlab("") +
  ggtitle('Users')
```

```{r}
ggsave('figures/users.png', type='cairo',
         width = 6, height = 6)
```

# Data Types Requested
```{r}
tmp <- the_data %>%
  select(ID, Data_Group) %>%
  unique()
tst <- xtabs(~Data_Group, tmp) %>%
  sort(decreasing = TRUE) %>%
  as_tibble()

cat("Number of Unique Data Records:\t")
sum(tst$n)
```

```{r}
tmp %>%
  filter(! is.na(Data_Group))  %>%
  mutate(Data_Group = fct_infreq(Data_Group)) %>%
   
  ggplot(aes(Data_Group)) +
  geom_bar(fill = "blue4") +
  theme(axis.text.x = element_text(angle = 90, size = 16,
                                   hjust = 1, vjust = 0.25)) +
  ylab('Count') +
  xlab("") +
  ggtitle("Data Types Requested or Implied")

```

```{r}
ggsave('figures/data.png', type='cairo',
         width = 6, height = 6)
```

# Model Extensions
```{r}
tmp <- the_data %>%
  select(ID, Extension_Category) %>%
  unique()
tst <- xtabs(~Extension_Category, tmp) %>%
  sort(decreasing = TRUE) %>%
  as_tibble()

cat("Number of Unique Extension Records:\t")
sum(tst$n)
```

```{r}
tmp %>%
  filter(! is.na(Extension_Category))  %>%
  mutate(Extension_Category = fct_infreq(Extension_Category)) %>%
   
  ggplot(aes(Extension_Category)) +
  geom_bar(fill = "blue4") +
  theme(axis.text.x = element_text(angle = 90, size = 16,
                                   hjust = 1, vjust = 0.25)) +
  ylab('Count') +
  xlab("") +
  ggtitle("Additional Models or Model Extensions Implied")
```

```{r}
ggsave('figures/models.png', type='cairo',
         width = 6, height = 6)
```

## What are the comments associated with the "Other" Category?
```{r}
the_data %>%
  filter(Extension_Category == "Other") %>%
  select(ID, Comment) %>%
  unique()%>%
  pull(Comment)
```

So the model extensions I classified as "Other" include:

*  Calculating tidal datums at specific locations,

*  Finding ways to model areas at high risk for eutrophication,

* Developing decision support tools for aquaculture siting and 
  permitting; and 
  
*  Modelling impact of rising seas on groundwater. 

Another ambiguous category are the comments I read as suggesting we expand the capabilities of the model in other ways:

```{r}
the_data %>%
  filter(Extension_Category == "Use Capabilities") %>%
  select(ID, Comment) %>%
  unique() %>%
  pull(Comment)
```

These include:

*  Incorporating sea level rise into forecasts of tidal elevations or
   tidal datums at habitat restoration locations or locations of vulnerable
   infrastructure
  
*  Developing versions of the model that can be used in a "what if" or 
   exploratory manner to build shared understanding when seeking policy
   solutions.
  
*  Retention of model output -- at least "now cast" and short-term forecasts -- 
   to allow forensic analysis of accidents and spills.
   
*  Ensure the model or its output can be coordinated with other data and other
   models.
   
*  Using the model to conduct simulations to improve understanding of ocean
   process and mechanism
   
*  Generating "digital twins" of the harbor.  The concept of 
   a digital twin generally implies the ability to run various kinds of 
   "what if" analyses.

# User Interface Ideas
```{r}
tmp <- the_data %>%
  select(ID, Interface_Category) %>%
  unique()
tst <- xtabs(~Interface_Category, tmp) %>%
  sort(decreasing = TRUE) %>%
  as_tibble()

cat("Number of Unique Interface Records:\t")
sum(tst$n)
```
```{r fig.height = 5, fig.width = 5}
tmp %>%
  filter(! is.na(Interface_Category))  %>%
  mutate(Interface_Category = fct_infreq(Interface_Category)) %>%
   
  ggplot(aes(Interface_Category)) +
  geom_bar(fill = "blue4") +

  theme(axis.text.x = element_text(angle = 90, size = 16,
                                   hjust = 1, vjust = 0.25)) +
  ylab('Count') +
  xlab("") +
  ggtitle("User Interface or Presentation Ideas")
```

```{r}
ggsave('figures/interfaces.png', type='cairo',
         width = 6, height = 6)
```


## What are the comments associated with the "Other" Category?
```{r}
the_data %>%
  filter(Interface_Category == "Other") %>%
  select(Comment) %>%
  unique() %>%
  pull()
```

# Model Performance Goals
```{r}
tmp <- the_data %>%
  select(ID, Performance_Apply, Performance_Criterion) %>%
  unique()
xtabs(~Performance_Criterion + Performance_Apply, data = tmp) %>%
  as_tibble() %>%
  pivot_wider(names_from = Performance_Criterion, values_from = n) %>%
  mutate(row_tot = rowSums(select(., `Accuracy`:`Temporal Resolution`))) %>%
  arrange(desc(row_tot))
```

```{r}
tmp %>%
  filter(! is.na(Performance_Apply))  %>%
  mutate(Performance_Apply = fct_infreq(Performance_Apply)) %>%
  mutate(Performance_Criterion = fct_infreq(Performance_Criterion)) %>%
   
  ggplot(aes(Performance_Apply)) +
  geom_bar(aes(fill = Performance_Criterion)) +
  theme(axis.text.x = element_text(angle = 90, size = 16,
                                   hjust = 1, vjust = 0.25)) +
  scale_fill_viridis_d(name = '') +
  ylab('Count') +
  xlab("") +
  ggtitle("Model Performance Suggestions")

```

```{r}
ggsave('figures/performance.png', type='cairo',
         width = 6, height = 6)
```

## What are the "Capabilities" Comments?
```{r}
the_data %>%
  filter(Performance_Criterion == "Capability") %>%
  select(ID, Comment) %>%
  unique() %>%
  pull(Comment)
```

Those can be categorized as:

1. Ability to conduct various kinds of exploratory, what if, or scenario-based
   exploration of actions;

2.  Incorporation of SLR into model forecasts;

3. Ability to track pollutants from many locations simultaneously, rather than 
   from just one location;

4. Ability to provide data for forensic analysis -- presumably past conditions;

5. Integration with other data and models.

# Monitoring Suggestions
```{r}
tmp <- the_data %>%
  select(ID, Monitoring_Data_Group) %>%
  unique()
tst <- xtabs(~Monitoring_Data_Group, tmp) %>%
  sort(decreasing = TRUE) %>%
  as_tibble()

cat("Number of Unique Extension Records:\t")
sum(tst$n)
```

```{r}
tmp %>%
  filter(! is.na(Monitoring_Data_Group))  %>%
  mutate(Monitoring_Data_Group = fct_infreq(Monitoring_Data_Group)) %>%
   
  ggplot(aes(Monitoring_Data_Group)) +
  geom_bar(fill = "blue4") +
  theme(axis.text.x = element_text(angle = 90, size = 16,
                                   hjust = 1, vjust = 0.25)) +
  ylab('Count') +
  xlab("") +
  ggtitle("Additional Models or Model Extensions Implied")

```

```{r}
ggsave('figures/monitoring.png', type='cairo',
         width = 6, height = 6)
```

## What are the "Not Specified" Comments?
```{r}
the_data %>%
  filter(Monitoring_Data_Group == "Not Specified") %>%
  select(ID, Comment) %>%
  unique() %>%
  pull(Comment)
```

These principally constitute comments on model validation and monitoring 
methods.
